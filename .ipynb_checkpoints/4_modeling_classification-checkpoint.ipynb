{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0f85a2-6cd4-4f7b-91e8-c126da5848d3",
   "metadata": {},
   "source": [
    "# Capstone 2: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00db90f-ffc7-496a-8154-b274df5ae21f",
   "metadata": {},
   "source": [
    "1. Functions to choose best models\n",
    "2. model testing and selection\n",
    "3. hyperparameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af36fdd-bc39-44f9-9392-a91af088bd54",
   "metadata": {},
   "source": [
    "# 1. Create the functions to choose the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74fc74-192a-4cc1-a7ed-ae5963b93aff",
   "metadata": {},
   "source": [
    "## Step 1: Load the dataset and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c059391-2e19-4637-8e50-752f9e024399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Revenue Growth</th>\n",
       "      <th>Cost of Revenue</th>\n",
       "      <th>Gross Profit</th>\n",
       "      <th>R&amp;D Expenses</th>\n",
       "      <th>SG&amp;A Expense</th>\n",
       "      <th>Operating Expenses</th>\n",
       "      <th>Operating Income</th>\n",
       "      <th>Interest Expense</th>\n",
       "      <th>Earnings before Tax</th>\n",
       "      <th>...</th>\n",
       "      <th>Sector_Communication Services</th>\n",
       "      <th>Sector_Consumer Cyclical</th>\n",
       "      <th>Sector_Consumer Defensive</th>\n",
       "      <th>Sector_Energy</th>\n",
       "      <th>Sector_Financial Services</th>\n",
       "      <th>Sector_Healthcare</th>\n",
       "      <th>Sector_Industrials</th>\n",
       "      <th>Sector_Real Estate</th>\n",
       "      <th>Sector_Technology</th>\n",
       "      <th>Sector_Utilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.440100e+10</td>\n",
       "      <td>-0.0713</td>\n",
       "      <td>3.903000e+10</td>\n",
       "      <td>3.537100e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.146100e+10</td>\n",
       "      <td>2.146100e+10</td>\n",
       "      <td>1.391000e+10</td>\n",
       "      <td>7.090000e+08</td>\n",
       "      <td>1.449400e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.734148e+09</td>\n",
       "      <td>1.1737</td>\n",
       "      <td>2.805625e+09</td>\n",
       "      <td>9.285226e+08</td>\n",
       "      <td>1.083303e+08</td>\n",
       "      <td>3.441414e+08</td>\n",
       "      <td>7.939267e+08</td>\n",
       "      <td>1.345959e+08</td>\n",
       "      <td>1.214869e+07</td>\n",
       "      <td>1.753823e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.837500e+10</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>7.813800e+10</td>\n",
       "      <td>2.023700e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.519600e+10</td>\n",
       "      <td>1.751200e+10</td>\n",
       "      <td>2.725000e+09</td>\n",
       "      <td>4.430000e+08</td>\n",
       "      <td>2.270000e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.552641e+10</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>1.820268e+10</td>\n",
       "      <td>7.323734e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.561162e+09</td>\n",
       "      <td>6.586482e+09</td>\n",
       "      <td>7.372520e+08</td>\n",
       "      <td>4.245910e+08</td>\n",
       "      <td>2.502180e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.790960e+10</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>1.153980e+10</td>\n",
       "      <td>6.369800e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.474300e+09</td>\n",
       "      <td>3.412400e+09</td>\n",
       "      <td>2.957400e+09</td>\n",
       "      <td>3.024000e+08</td>\n",
       "      <td>2.707700e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Revenue  Revenue Growth  Cost of Revenue  Gross Profit  R&D Expenses  \\\n",
       "0  7.440100e+10         -0.0713     3.903000e+10  3.537100e+10  0.000000e+00   \n",
       "1  3.734148e+09          1.1737     2.805625e+09  9.285226e+08  1.083303e+08   \n",
       "2  9.837500e+10          0.0182     7.813800e+10  2.023700e+10  0.000000e+00   \n",
       "3  2.552641e+10          0.0053     1.820268e+10  7.323734e+09  0.000000e+00   \n",
       "4  1.790960e+10          0.0076     1.153980e+10  6.369800e+09  0.000000e+00   \n",
       "\n",
       "   SG&A Expense  Operating Expenses  Operating Income  Interest Expense  \\\n",
       "0  2.146100e+10        2.146100e+10      1.391000e+10      7.090000e+08   \n",
       "1  3.441414e+08        7.939267e+08      1.345959e+08      1.214869e+07   \n",
       "2  1.519600e+10        1.751200e+10      2.725000e+09      4.430000e+08   \n",
       "3  6.561162e+09        6.586482e+09      7.372520e+08      4.245910e+08   \n",
       "4  3.474300e+09        3.412400e+09      2.957400e+09      3.024000e+08   \n",
       "\n",
       "   Earnings before Tax  ...  Sector_Communication Services  \\\n",
       "0         1.449400e+10  ...                              0   \n",
       "1         1.753823e+08  ...                              0   \n",
       "2         2.270000e+09  ...                              0   \n",
       "3         2.502180e+08  ...                              0   \n",
       "4         2.707700e+09  ...                              0   \n",
       "\n",
       "   Sector_Consumer Cyclical  Sector_Consumer Defensive  Sector_Energy  \\\n",
       "0                         0                          1              0   \n",
       "1                         0                          1              0   \n",
       "2                         0                          1              0   \n",
       "3                         0                          1              0   \n",
       "4                         0                          1              0   \n",
       "\n",
       "   Sector_Financial Services  Sector_Healthcare  Sector_Industrials  \\\n",
       "0                          0                  0                   0   \n",
       "1                          0                  0                   0   \n",
       "2                          0                  0                   0   \n",
       "3                          0                  0                   0   \n",
       "4                          0                  0                   0   \n",
       "\n",
       "   Sector_Real Estate  Sector_Technology  Sector_Utilities  \n",
       "0                   0                  0                 0  \n",
       "1                   0                  0                 0  \n",
       "2                   0                  0                 0  \n",
       "3                   0                  0                 0  \n",
       "4                   0                  0                 0  \n",
       "\n",
       "[5 rows x 196 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier, Ridge, Lasso, ElasticNet \n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the dataset\n",
    "outliers = '../data/modeling_data/with_outliers.csv'\n",
    "\n",
    "df = pd.read_csv(outliers)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78047490-cc6c-467f-8032-6081c5f56041",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess the Data\n",
    "We'll create functions to drop unnecessary columns, standardize the features, and split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfcf7cea-fc27-4e1c-adb6-df33b42a6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_variable):\n",
    "    # Drop unnecessary dependent variables and Symbol column\n",
    "    drop_columns = ['PRICE VAR [%]', 'Alpha', 'Alpha_gt_3', 'Alpha_gt_5', 'Alpha_gt_10', 'Symbol']\n",
    "    df = df.drop(columns=[col for col in drop_columns if col != target_variable])\n",
    "    \n",
    "    # Drop non-numeric columns\n",
    "    non_numeric_columns = df.select_dtypes(include=['object']).columns\n",
    "    df = df.drop(columns=non_numeric_columns)\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    X = df.drop(columns=[target_variable])\n",
    "    y = df[target_variable]\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    return X_scaled, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6b111f-50f6-44d7-8557-94a4998bda8d",
   "metadata": {},
   "source": [
    "## Step 3: Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e503c5fd-c49d-4bd4-a091-8679eccbdad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    val_split = val_size / (1 - test_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_split, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cefbcb-a414-4716-9b89-4b482a99ddeb",
   "metadata": {},
   "source": [
    "## Step 4: Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16d27aa8-a8e9-489b-8321-cc5af1404578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca(X, n_components=None, variance_ratio=None):\n",
    "    if variance_ratio is not None:\n",
    "        pca = PCA(n_components=variance_ratio)\n",
    "    else:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        \n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8670e307-a85c-4614-898a-7e6d6c6d5e11",
   "metadata": {},
   "source": [
    "## Step 5: Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac244b7e-0343-4a06-be32-32e5683c585e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def model_data(model, X_train, y_train, X_val, y_val, X_test, y_test, problem_type='regression'):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation and test sets\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    if problem_type == 'regression':\n",
    "        # Calculate metrics for regression problems\n",
    "        val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "        test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "        val_rmse = val_mse ** 0.5\n",
    "        test_rmse = test_mse ** 0.5\n",
    "        val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        val_r2 = r2_score(y_val, y_val_pred)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        return model, val_mse, test_mse, val_rmse, test_rmse, val_mae, test_mae, val_r2, test_r2\n",
    "        \n",
    "    elif problem_type == 'classification':\n",
    "        # Calculate metrics for classification problems\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        val_precision = precision_score(y_val, y_val_pred)\n",
    "        test_precision = precision_score(y_test, y_test_pred)\n",
    "        val_recall = recall_score(y_val, y_val_pred)\n",
    "        test_recall = recall_score(y_test, y_test_pred)\n",
    "        val_f1 = f1_score(y_val, y_val_pred)\n",
    "        test_f1 = f1_score(y_test, y_test_pred)\n",
    "        val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "        test_auc = roc_auc_score(y_test, y_test_pred)\n",
    "        \n",
    "        return model, val_accuracy, test_accuracy, val_precision, test_precision, val_recall, test_recall, val_f1, test_f1, val_auc, test_auc\n",
    "    else:\n",
    "        raise ValueError(\"Invalid problem_type. Use 'regression' or 'classification'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a953e3-ade5-4e5d-8a6b-954ae558ae8b",
   "metadata": {},
   "source": [
    "# 2. Model testting and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe17ac8-c84a-42ef-9552-64823c476cc2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Hands on modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7631bb9-c902-41cf-9cbe-450f9e79bee5",
   "metadata": {},
   "source": [
    "## Prediction Models (dependent variable: Alphas over S&P500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defee163-2e96-42b9-8282-8b2c46d74395",
   "metadata": {},
   "source": [
    "###  **1.1 Modeling with all features including outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e01538ed-86cb-42f8-b65d-f9af7c3650fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'Alpha_gt_3'\n",
    "X_scaled, y = preprocess_data(df, target_variable)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e181f8f-e7de-451a-bf24-88dd05324fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "  Validation Accuracy: 0.6120600414078675\n",
      "  Test Accuracy: 0.5670289855072463\n",
      "  Validation Precision: 0.4720670391061452\n",
      "  Test Precision: 0.3888888888888889\n",
      "  Validation Recall: 0.11426639621365788\n",
      "  Test Recall: 0.09251101321585903\n",
      "  Validation F1 Score: 0.1839956450734894\n",
      "  Test F1 Score: 0.1494661921708185\n",
      "  Validation AUC: 0.517510556597395\n",
      "  Test AUC: 0.49548627583869875\n",
      "\n",
      "\n",
      "Model: Random Forest Classifier\n",
      "  Validation Accuracy: 0.6493271221532091\n",
      "  Test Accuracy: 0.6485507246376812\n",
      "  Validation Precision: 0.5895953757225434\n",
      "  Test Precision: 0.6571428571428571\n",
      "  Validation Recall: 0.27586206896551724\n",
      "  Test Recall: 0.3039647577092511\n",
      "  Validation F1 Score: 0.375863657300783\n",
      "  Test F1 Score: 0.41566265060240964\n",
      "  Validation AUC: 0.5783922504156727\n",
      "  Test AUC: 0.5965977634700101\n",
      "\n",
      "\n",
      "Model: Decision Tree Classifier\n",
      "  Validation Accuracy: 0.5913561076604554\n",
      "  Test Accuracy: 0.5597826086956522\n",
      "  Validation Precision: 0.46630727762803237\n",
      "  Test Precision: 0.4636363636363636\n",
      "  Validation Recall: 0.4678837052062204\n",
      "  Test Recall: 0.44933920704845814\n",
      "  Validation F1 Score: 0.4670941613229835\n",
      "  Test F1 Score: 0.4563758389261745\n",
      "  Validation AUC: 0.5679041167540536\n",
      "  Test AUC: 0.5431311419857675\n",
      "\n",
      "\n",
      "Model: Gradient Boosting Classifier\n",
      "  Validation Accuracy: 0.6640786749482401\n",
      "  Test Accuracy: 0.6394927536231884\n",
      "  Validation Precision: 0.6248275862068966\n",
      "  Test Precision: 0.6372549019607843\n",
      "  Validation Recall: 0.30628803245436104\n",
      "  Test Recall: 0.28634361233480177\n",
      "  Validation F1 Score: 0.411070780399274\n",
      "  Test F1 Score: 0.3951367781155015\n",
      "  Validation AUC: 0.5961209554305349\n",
      "  Test AUC: 0.5862487292443239\n",
      "\n",
      "\n",
      "Model: AdaBoost Classifier\n",
      "  Validation Accuracy: 0.6415631469979296\n",
      "  Test Accuracy: 0.625\n",
      "  Validation Precision: 0.5611979166666666\n",
      "  Test Precision: 0.5943396226415094\n",
      "  Validation Recall: 0.2914131169709263\n",
      "  Test Recall: 0.2775330396475771\n",
      "  Validation F1 Score: 0.38362260792167335\n",
      "  Test F1 Score: 0.3783783783783784\n",
      "  Validation AUC: 0.575056663307266\n",
      "  Test AUC: 0.5726126736699423\n",
      "\n",
      "\n",
      "Model: Support Vector Classifier\n",
      "  Validation Accuracy: 0.6231884057971014\n",
      "  Test Accuracy: 0.5942028985507246\n",
      "  Validation Precision: 0.7555555555555555\n",
      "  Test Precision: 0.8\n",
      "  Validation Recall: 0.022988505747126436\n",
      "  Test Recall: 0.01762114537444934\n",
      "  Validation F1 Score: 0.04461942257217848\n",
      "  Test F1 Score: 0.034482758620689655\n",
      "  Validation AUC: 0.509188173208993\n",
      "  Test AUC: 0.507272111148763\n",
      "\n",
      "\n",
      "Model: K-Nearest Neighbors Classifier\n",
      "  Validation Accuracy: 0.6151656314699793\n",
      "  Test Accuracy: 0.6177536231884058\n",
      "  Validation Precision: 0.4968\n",
      "  Test Precision: 0.5449438202247191\n",
      "  Validation Recall: 0.4198782961460446\n",
      "  Test Recall: 0.42731277533039647\n",
      "  Validation F1 Score: 0.45511176255038477\n",
      "  Test F1 Score: 0.47901234567901235\n",
      "  Validation AUC: 0.578073319980779\n",
      "  Test AUC: 0.5890410030498135\n",
      "\n",
      "\n",
      "Model: Ridge Classifier\n",
      "  Validation Accuracy: 0.6120600414078675\n",
      "  Test Accuracy: 0.5742753623188406\n",
      "  Validation Precision: 0.46794871794871795\n",
      "  Test Precision: 0.42\n",
      "  Validation Recall: 0.09871534820824882\n",
      "  Test Recall: 0.09251101321585903\n",
      "  Validation F1 Score: 0.16303740926856505\n",
      "  Test F1 Score: 0.15162454873646208\n",
      "  Validation AUC: 0.5145568355297009\n",
      "  Test AUC: 0.5016401219925449\n",
      "\n",
      "\n",
      "Model: Random Forest Regressor\n",
      "  Validation MSE: 0.20800842221925864\n",
      "  Test MSE: 0.2099563784260081\n",
      "  Validation RMSE: 0.4560794034148644\n",
      "  Test RMSE: 0.4582099719844692\n",
      "  Validation MAE: 0.4129232783693187\n",
      "  Test MAE: 0.41651923740510705\n",
      "  Validation R^2: 0.1195623662339268\n",
      "  Test R^2: 0.13284244890656216\n",
      "\n",
      "\n",
      "Model: Decision Tree Regressor\n",
      "  Validation MSE: 0.41018949850471587\n",
      "  Test MSE: 0.4438405797101449\n",
      "  Validation RMSE: 0.6404603801209844\n",
      "  Test RMSE: 0.6662136141735209\n",
      "  Validation MAE: 0.4104986197377502\n",
      "  Test MAE: 0.4438405797101449\n",
      "  Validation R^2: -0.7362098496113068\n",
      "  Test R^2: -0.833141308031176\n",
      "\n",
      "\n",
      "Model: Gradient Boosting Regressor\n",
      "  Validation MSE: 0.21276309080534594\n",
      "  Test MSE: 0.21846966715950683\n",
      "  Validation RMSE: 0.46126249663867747\n",
      "  Test RMSE: 0.4674073888584848\n",
      "  Validation MAE: 0.43286731166880843\n",
      "  Test MAE: 0.43707342332818055\n",
      "  Validation R^2: 0.09943727170836003\n",
      "  Test R^2: 0.09768103744940193\n",
      "\n",
      "\n",
      "Model: AdaBoost Regressor\n",
      "  Validation MSE: 0.22571988768950987\n",
      "  Test MSE: 0.22710194415745916\n",
      "  Validation RMSE: 0.47509987127919734\n",
      "  Test RMSE: 0.4765521421182148\n",
      "  Validation MAE: 0.44937845181942937\n",
      "  Test MAE: 0.4488976844194174\n",
      "  Validation R^2: 0.044595013552792384\n",
      "  Test R^2: 0.0620281830897399\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define models to test with their corresponding problem types\n",
    "models = {\n",
    "    \"Logistic Regression\": (LogisticRegression(max_iter=1000, random_state=42), \"classification\"),  # Increased max_iter\n",
    "    \"Random Forest Classifier\": (RandomForestClassifier(n_estimators=100, random_state=42), \"classification\"),\n",
    "    \"Decision Tree Classifier\": (DecisionTreeClassifier(random_state=42), \"classification\"),\n",
    "    \"Gradient Boosting Classifier\": (GradientBoostingClassifier(random_state=42), \"classification\"),\n",
    "    \"AdaBoost Classifier\": (AdaBoostClassifier(random_state=42), \"classification\"),\n",
    "    \"Support Vector Classifier\": (SVC(), \"classification\"),\n",
    "    \"K-Nearest Neighbors Classifier\": (KNeighborsClassifier(), \"classification\"),\n",
    "    \"Ridge Classifier\": (RidgeClassifier(), \"classification\"),\n",
    "    \"Random Forest Regressor\": (RandomForestRegressor(n_estimators=100, random_state=42), \"regression\"),\n",
    "    \"Decision Tree Regressor\": (DecisionTreeRegressor(random_state=42), \"regression\"),\n",
    "    \"Gradient Boosting Regressor\": (GradientBoostingRegressor(random_state=42), \"regression\"),\n",
    "    \"AdaBoost Regressor\": (AdaBoostRegressor(random_state=42), \"regression\"),\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, (model, problem_type) in models.items():\n",
    "    if problem_type == \"classification\":\n",
    "        model_fitted, val_accuracy, test_accuracy, val_precision, test_precision, val_recall, test_recall, val_f1, test_f1, val_auc, test_auc = model_data(\n",
    "            model, X_train, y_train, X_val, y_val, X_test, y_test, problem_type=problem_type)\n",
    "        \n",
    "        # Store classification results\n",
    "        results[name] = {\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Test Accuracy\": test_accuracy,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Test Precision\": test_precision,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Test Recall\": test_recall,\n",
    "            \"Validation F1 Score\": val_f1,\n",
    "            \"Test F1 Score\": test_f1,\n",
    "            \"Validation AUC\": val_auc,\n",
    "            \"Test AUC\": test_auc\n",
    "        }\n",
    "    elif problem_type == \"regression\":\n",
    "        model_fitted, val_mse, test_mse, val_rmse, test_rmse, val_mae, test_mae, val_r2, test_r2 = model_data(\n",
    "            model, X_train, y_train, X_val, y_val, X_test, y_test, problem_type=problem_type)\n",
    "        \n",
    "        # Store regression results\n",
    "        results[name] = {\n",
    "            \"Validation MSE\": val_mse,\n",
    "            \"Test MSE\": test_mse,\n",
    "            \"Validation RMSE\": val_rmse,\n",
    "            \"Test RMSE\": test_rmse,\n",
    "            \"Validation MAE\": val_mae,\n",
    "            \"Test MAE\": test_mae,\n",
    "            \"Validation R^2\": val_r2,\n",
    "            \"Test R^2\": test_r2\n",
    "        }\n",
    "\n",
    "# Print results\n",
    "for name, result in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    for metric, value in result.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aabb0c8-1596-499e-9a7f-66f7854ed852",
   "metadata": {},
   "source": [
    "### Classification Model Performance Comparison\n",
    "\n",
    "In this analysis, we compare the performance of different classification models using the `alpha_gt_3` target variable. The models evaluated include Logistic Regression, Random Forest, Decision Tree, Gradient Boosting, AdaBoost, Support Vector Classifier, K-Nearest Neighbors, and Ridge Classifier.\n",
    "\n",
    "#### Summary of Key Metrics\n",
    "\n",
    "1. **Accuracy:** Reflects the proportion of correct predictions among the total predictions. \n",
    "   - **Highest Accuracy:** **Gradient Boosting Classifier** \n",
    "     - Validation Accuracy: **66.41%**\n",
    "     - Test Accuracy: **63.95%**\n",
    "\n",
    "2. **Precision:** Measures the accuracy of positive predictions (i.e., the proportion of true positives among the predicted positives).\n",
    "   - **Highest Precision:** **Support Vector Classifier**\n",
    "     - Validation Precision: **75.56%**\n",
    "     - Test Precision: **80.00%**\n",
    "\n",
    "3. **Recall:** Indicates how many of the actual positives were correctly identified (i.e., true positive rate).\n",
    "   - **Highest Recall:** **Decision Tree Classifier**\n",
    "     - Validation Recall: **46.79%**\n",
    "     - Test Recall: **44.93%**\n",
    "\n",
    "4. **F1 Score:** The harmonic mean of Precision and Recall, providing a balance between the two.\n",
    "   - **Highest F1 Score:** **Random Forest Classifier**\n",
    "     - Validation F1 Score: **37.59%**\n",
    "     - Test F1 Score: **41.57%**\n",
    "\n",
    "5. **AUC (Area Under the Curve):** Measures the ability of the classifier to distinguish between classes.\n",
    "   - **Highest AUC:** **Random Forest Classifier**\n",
    "     - Validation AUC: **57.84%**\n",
    "     - Test AUC: **59.66%**\n",
    "\n",
    "#### Observations\n",
    "\n",
    "- **Random Forest Classifier** generally performed well across most metrics, particularly in terms of F1 Score and AUC, which are critical for imbalanced datasets.\n",
    "- **Gradient Boosting Classifier** also showed strong overall performance, especially in Accuracy.\n",
    "- **Support Vector Classifier** achieved high Precision but had very low Recall, indicating that it was highly conservative in predicting the positive class.\n",
    "- **K-Nearest Neighbors** and **Decision Tree Classifier** provided a balance between Precision and Recall but did not excel in any specific area.\n",
    "\n",
    "#### Recommendations\n",
    "\n",
    "- **Model Selection:** Based on these results, **Random Forest Classifier** and **Gradient Boosting Classifier** are strong candidates for further tuning and potential deployment, as they provide a good balance of accuracy and robustness across different metrics.\n",
    "- **Hyperparameter Tuning:** Consider performing hyperparameter tuning on Random Forest and Gradient Boosting to potentially improve their performance further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50582443-2680-4d18-ba43-97fa95b65922",
   "metadata": {},
   "source": [
    "###  **1.2 Modeling with all features dropping outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64a1feb2-1ec5-43a9-9dbe-d9a72aec4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_outliers = '../data/modeling_data/drop_outliers.csv'\n",
    "df_drop = pd.read_csv(drop_outliers)\n",
    "\n",
    "target_variable = 'Alpha_gt_3'\n",
    "X_scaled, y = preprocess_data(df_drop, target_variable)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cc2db4c-e392-4366-a5b3-7c593d2a92fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "  Validation Accuracy: 0.6120600414078675\n",
      "  Test Accuracy: 0.5670289855072463\n",
      "  Validation Precision: 0.4720670391061452\n",
      "  Test Precision: 0.3888888888888889\n",
      "  Validation Recall: 0.11426639621365788\n",
      "  Test Recall: 0.09251101321585903\n",
      "  Validation F1 Score: 0.1839956450734894\n",
      "  Test F1 Score: 0.1494661921708185\n",
      "  Validation AUC: 0.517510556597395\n",
      "  Test AUC: 0.49548627583869875\n",
      "\n",
      "\n",
      "Model: Random Forest Classifier\n",
      "  Validation Accuracy: 0.6493271221532091\n",
      "  Test Accuracy: 0.6485507246376812\n",
      "  Validation Precision: 0.5895953757225434\n",
      "  Test Precision: 0.6571428571428571\n",
      "  Validation Recall: 0.27586206896551724\n",
      "  Test Recall: 0.3039647577092511\n",
      "  Validation F1 Score: 0.375863657300783\n",
      "  Test F1 Score: 0.41566265060240964\n",
      "  Validation AUC: 0.5783922504156727\n",
      "  Test AUC: 0.5965977634700101\n",
      "\n",
      "\n",
      "Model: Decision Tree Classifier\n",
      "  Validation Accuracy: 0.5913561076604554\n",
      "  Test Accuracy: 0.5597826086956522\n",
      "  Validation Precision: 0.46630727762803237\n",
      "  Test Precision: 0.4636363636363636\n",
      "  Validation Recall: 0.4678837052062204\n",
      "  Test Recall: 0.44933920704845814\n",
      "  Validation F1 Score: 0.4670941613229835\n",
      "  Test F1 Score: 0.4563758389261745\n",
      "  Validation AUC: 0.5679041167540536\n",
      "  Test AUC: 0.5431311419857675\n",
      "\n",
      "\n",
      "Model: Gradient Boosting Classifier\n",
      "  Validation Accuracy: 0.6640786749482401\n",
      "  Test Accuracy: 0.6394927536231884\n",
      "  Validation Precision: 0.6248275862068966\n",
      "  Test Precision: 0.6372549019607843\n",
      "  Validation Recall: 0.30628803245436104\n",
      "  Test Recall: 0.28634361233480177\n",
      "  Validation F1 Score: 0.411070780399274\n",
      "  Test F1 Score: 0.3951367781155015\n",
      "  Validation AUC: 0.5961209554305349\n",
      "  Test AUC: 0.5862487292443239\n",
      "\n",
      "\n",
      "Model: AdaBoost Classifier\n",
      "  Validation Accuracy: 0.6415631469979296\n",
      "  Test Accuracy: 0.625\n",
      "  Validation Precision: 0.5611979166666666\n",
      "  Test Precision: 0.5943396226415094\n",
      "  Validation Recall: 0.2914131169709263\n",
      "  Test Recall: 0.2775330396475771\n",
      "  Validation F1 Score: 0.38362260792167335\n",
      "  Test F1 Score: 0.3783783783783784\n",
      "  Validation AUC: 0.575056663307266\n",
      "  Test AUC: 0.5726126736699423\n",
      "\n",
      "\n",
      "Model: Support Vector Classifier\n",
      "  Validation Accuracy: 0.6231884057971014\n",
      "  Test Accuracy: 0.5942028985507246\n",
      "  Validation Precision: 0.7555555555555555\n",
      "  Test Precision: 0.8\n",
      "  Validation Recall: 0.022988505747126436\n",
      "  Test Recall: 0.01762114537444934\n",
      "  Validation F1 Score: 0.04461942257217848\n",
      "  Test F1 Score: 0.034482758620689655\n",
      "  Validation AUC: 0.509188173208993\n",
      "  Test AUC: 0.507272111148763\n",
      "\n",
      "\n",
      "Model: K-Nearest Neighbors Classifier\n",
      "  Validation Accuracy: 0.6151656314699793\n",
      "  Test Accuracy: 0.6177536231884058\n",
      "  Validation Precision: 0.4968\n",
      "  Test Precision: 0.5449438202247191\n",
      "  Validation Recall: 0.4198782961460446\n",
      "  Test Recall: 0.42731277533039647\n",
      "  Validation F1 Score: 0.45511176255038477\n",
      "  Test F1 Score: 0.47901234567901235\n",
      "  Validation AUC: 0.578073319980779\n",
      "  Test AUC: 0.5890410030498135\n",
      "\n",
      "\n",
      "Model: Ridge Classifier\n",
      "  Validation Accuracy: 0.6120600414078675\n",
      "  Test Accuracy: 0.5742753623188406\n",
      "  Validation Precision: 0.46794871794871795\n",
      "  Test Precision: 0.42\n",
      "  Validation Recall: 0.09871534820824882\n",
      "  Test Recall: 0.09251101321585903\n",
      "  Validation F1 Score: 0.16303740926856505\n",
      "  Test F1 Score: 0.15162454873646208\n",
      "  Validation AUC: 0.5145568355297009\n",
      "  Test AUC: 0.5016401219925449\n",
      "\n",
      "\n",
      "Model: Random Forest Regressor\n",
      "  Validation MSE: 0.20800842221925864\n",
      "  Test MSE: 0.2099563784260081\n",
      "  Validation RMSE: 0.4560794034148644\n",
      "  Test RMSE: 0.4582099719844692\n",
      "  Validation MAE: 0.4129232783693187\n",
      "  Test MAE: 0.41651923740510705\n",
      "  Validation R^2: 0.1195623662339268\n",
      "  Test R^2: 0.13284244890656216\n",
      "\n",
      "\n",
      "Model: Decision Tree Regressor\n",
      "  Validation MSE: 0.41018949850471587\n",
      "  Test MSE: 0.4438405797101449\n",
      "  Validation RMSE: 0.6404603801209844\n",
      "  Test RMSE: 0.6662136141735209\n",
      "  Validation MAE: 0.4104986197377502\n",
      "  Test MAE: 0.4438405797101449\n",
      "  Validation R^2: -0.7362098496113068\n",
      "  Test R^2: -0.833141308031176\n",
      "\n",
      "\n",
      "Model: Gradient Boosting Regressor\n",
      "  Validation MSE: 0.21276309080534594\n",
      "  Test MSE: 0.21846966715950683\n",
      "  Validation RMSE: 0.46126249663867747\n",
      "  Test RMSE: 0.4674073888584848\n",
      "  Validation MAE: 0.43286731166880843\n",
      "  Test MAE: 0.43707342332818055\n",
      "  Validation R^2: 0.09943727170836003\n",
      "  Test R^2: 0.09768103744940193\n",
      "\n",
      "\n",
      "Model: AdaBoost Regressor\n",
      "  Validation MSE: 0.22571988768950987\n",
      "  Test MSE: 0.22710194415745916\n",
      "  Validation RMSE: 0.47509987127919734\n",
      "  Test RMSE: 0.4765521421182148\n",
      "  Validation MAE: 0.44937845181942937\n",
      "  Test MAE: 0.4488976844194174\n",
      "  Validation R^2: 0.044595013552792384\n",
      "  Test R^2: 0.0620281830897399\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define models to test with their corresponding problem types\n",
    "models = {\n",
    "    \"Logistic Regression\": (LogisticRegression(max_iter=1000, random_state=42), \"classification\"),  # Increased max_iter\n",
    "    \"Random Forest Classifier\": (RandomForestClassifier(n_estimators=100, random_state=42), \"classification\"),\n",
    "    \"Decision Tree Classifier\": (DecisionTreeClassifier(random_state=42), \"classification\"),\n",
    "    \"Gradient Boosting Classifier\": (GradientBoostingClassifier(random_state=42), \"classification\"),\n",
    "    \"AdaBoost Classifier\": (AdaBoostClassifier(random_state=42), \"classification\"),\n",
    "    \"Support Vector Classifier\": (SVC(), \"classification\"),\n",
    "    \"K-Nearest Neighbors Classifier\": (KNeighborsClassifier(), \"classification\"),\n",
    "    \"Ridge Classifier\": (RidgeClassifier(), \"classification\"),\n",
    "    \"Random Forest Regressor\": (RandomForestRegressor(n_estimators=100, random_state=42), \"regression\"),\n",
    "    \"Decision Tree Regressor\": (DecisionTreeRegressor(random_state=42), \"regression\"),\n",
    "    \"Gradient Boosting Regressor\": (GradientBoostingRegressor(random_state=42), \"regression\"),\n",
    "    \"AdaBoost Regressor\": (AdaBoostRegressor(random_state=42), \"regression\"),\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, (model, problem_type) in models.items():\n",
    "    if problem_type == \"classification\":\n",
    "        model_fitted, val_accuracy, test_accuracy, val_precision, test_precision, val_recall, test_recall, val_f1, test_f1, val_auc, test_auc = model_data(\n",
    "            model, X_train, y_train, X_val, y_val, X_test, y_test, problem_type=problem_type)\n",
    "        \n",
    "        # Store classification results\n",
    "        results[name] = {\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Test Accuracy\": test_accuracy,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Test Precision\": test_precision,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Test Recall\": test_recall,\n",
    "            \"Validation F1 Score\": val_f1,\n",
    "            \"Test F1 Score\": test_f1,\n",
    "            \"Validation AUC\": val_auc,\n",
    "            \"Test AUC\": test_auc\n",
    "        }\n",
    "    elif problem_type == \"regression\":\n",
    "        model_fitted, val_mse, test_mse, val_rmse, test_rmse, val_mae, test_mae, val_r2, test_r2 = model_data(\n",
    "            model, X_train, y_train, X_val, y_val, X_test, y_test, problem_type=problem_type)\n",
    "        \n",
    "        # Store regression results\n",
    "        results[name] = {\n",
    "            \"Validation MSE\": val_mse,\n",
    "            \"Test MSE\": test_mse,\n",
    "            \"Validation RMSE\": val_rmse,\n",
    "            \"Test RMSE\": test_rmse,\n",
    "            \"Validation MAE\": val_mae,\n",
    "            \"Test MAE\": test_mae,\n",
    "            \"Validation R^2\": val_r2,\n",
    "            \"Test R^2\": test_r2\n",
    "        }\n",
    "\n",
    "# Print results\n",
    "for name, result in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    for metric, value in result.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ffb2a6-4b2b-464d-ab5f-06243ac1eaec",
   "metadata": {},
   "source": [
    "### Classification Model Performance Comparison After Removing Outliers\n",
    "\n",
    "In this analysis, we compare the performance of different classification models using the `alpha_gt_3` target variable. The dataset was preprocessed to remove outliers using the 1st and 99th quantiles (`quantile_1=0.01`, `quantile_3=0.99`). The models evaluated include Logistic Regression, Random Forest, Decision Tree, Gradient Boosting, AdaBoost, Support Vector Classifier, K-Nearest Neighbors, and Ridge Classifier.\n",
    "\n",
    "#### Observations\n",
    "\n",
    "- **Random Forest Classifier** continues to perform well across most metrics, particularly in terms of F1 Score and AUC, which are critical for imbalanced datasets.\n",
    "- **Gradient Boosting Classifier** also shows strong overall performance, especially in Accuracy.\n",
    "- **Support Vector Classifier** achieved high Precision but had very low Recall, indicating that it was highly conservative in predicting the positive class.\n",
    "- **K-Nearest Neighbors** and **Decision Tree Classifier** provided a balance between Precision and Recall but did not excel in any specific area.\n",
    "\n",
    "#### Recommendations\n",
    "\n",
    "- **Model Selection:** Based on these results, **Random Forest Classifier** and **Gradient Boosting Classifier** remain strong candidates for further tuning and potential deployment, as they provide a good balance of accuracy and robustness across different metrics.\n",
    "- **Hyperparameter Tuning:** Consider performing hyperparameter tuning on Random Forest and Gradient Boosting to potentially improve their performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb29298-18f0-4524-9f6e-6caaee9f36dd",
   "metadata": {},
   "source": [
    "###  **1.3 Modeling with all features capping outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59daa800-fb55-44ba-8fbd-28297ef0c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_outliers = '../data/modeling_data/cap_outliers.csv'\n",
    "df_cap = pd.read_csv(cap_outliers)\n",
    "\n",
    "target_variable = 'Alpha_gt_3'\n",
    "X_scaled, y = preprocess_data(df_cap, target_variable)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9570fd4-13be-445b-8d89-bd89668b33d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "  Validation Accuracy: 0.6120600414078675\n",
      "  Test Accuracy: 0.5670289855072463\n",
      "  Validation Precision: 0.4720670391061452\n",
      "  Test Precision: 0.3888888888888889\n",
      "  Validation Recall: 0.11426639621365788\n",
      "  Test Recall: 0.09251101321585903\n",
      "  Validation F1 Score: 0.1839956450734894\n",
      "  Test F1 Score: 0.1494661921708185\n",
      "  Validation AUC: 0.517510556597395\n",
      "  Test AUC: 0.49548627583869875\n",
      "\n",
      "\n",
      "Model: Random Forest Classifier\n",
      "  Validation Accuracy: 0.6493271221532091\n",
      "  Test Accuracy: 0.6485507246376812\n",
      "  Validation Precision: 0.5895953757225434\n",
      "  Test Precision: 0.6571428571428571\n",
      "  Validation Recall: 0.27586206896551724\n",
      "  Test Recall: 0.3039647577092511\n",
      "  Validation F1 Score: 0.375863657300783\n",
      "  Test F1 Score: 0.41566265060240964\n",
      "  Validation AUC: 0.5783922504156727\n",
      "  Test AUC: 0.5965977634700101\n",
      "\n",
      "\n",
      "Model: Decision Tree Classifier\n",
      "  Validation Accuracy: 0.5913561076604554\n",
      "  Test Accuracy: 0.5597826086956522\n",
      "  Validation Precision: 0.46630727762803237\n",
      "  Test Precision: 0.4636363636363636\n",
      "  Validation Recall: 0.4678837052062204\n",
      "  Test Recall: 0.44933920704845814\n",
      "  Validation F1 Score: 0.4670941613229835\n",
      "  Test F1 Score: 0.4563758389261745\n",
      "  Validation AUC: 0.5679041167540536\n",
      "  Test AUC: 0.5431311419857675\n",
      "\n",
      "\n",
      "Model: Gradient Boosting Classifier\n",
      "  Validation Accuracy: 0.6640786749482401\n",
      "  Test Accuracy: 0.6394927536231884\n",
      "  Validation Precision: 0.6248275862068966\n",
      "  Test Precision: 0.6372549019607843\n",
      "  Validation Recall: 0.30628803245436104\n",
      "  Test Recall: 0.28634361233480177\n",
      "  Validation F1 Score: 0.411070780399274\n",
      "  Test F1 Score: 0.3951367781155015\n",
      "  Validation AUC: 0.5961209554305349\n",
      "  Test AUC: 0.5862487292443239\n",
      "\n",
      "\n",
      "Model: AdaBoost Classifier\n",
      "  Validation Accuracy: 0.6415631469979296\n",
      "  Test Accuracy: 0.625\n",
      "  Validation Precision: 0.5611979166666666\n",
      "  Test Precision: 0.5943396226415094\n",
      "  Validation Recall: 0.2914131169709263\n",
      "  Test Recall: 0.2775330396475771\n",
      "  Validation F1 Score: 0.38362260792167335\n",
      "  Test F1 Score: 0.3783783783783784\n",
      "  Validation AUC: 0.575056663307266\n",
      "  Test AUC: 0.5726126736699423\n",
      "\n",
      "\n",
      "Model: Support Vector Classifier\n",
      "  Validation Accuracy: 0.6231884057971014\n",
      "  Test Accuracy: 0.5942028985507246\n",
      "  Validation Precision: 0.7555555555555555\n",
      "  Test Precision: 0.8\n",
      "  Validation Recall: 0.022988505747126436\n",
      "  Test Recall: 0.01762114537444934\n",
      "  Validation F1 Score: 0.04461942257217848\n",
      "  Test F1 Score: 0.034482758620689655\n",
      "  Validation AUC: 0.509188173208993\n",
      "  Test AUC: 0.507272111148763\n",
      "\n",
      "\n",
      "Model: K-Nearest Neighbors Classifier\n",
      "  Validation Accuracy: 0.6151656314699793\n",
      "  Test Accuracy: 0.6177536231884058\n",
      "  Validation Precision: 0.4968\n",
      "  Test Precision: 0.5449438202247191\n",
      "  Validation Recall: 0.4198782961460446\n",
      "  Test Recall: 0.42731277533039647\n",
      "  Validation F1 Score: 0.45511176255038477\n",
      "  Test F1 Score: 0.47901234567901235\n",
      "  Validation AUC: 0.578073319980779\n",
      "  Test AUC: 0.5890410030498135\n",
      "\n",
      "\n",
      "Model: Ridge Classifier\n",
      "  Validation Accuracy: 0.6120600414078675\n",
      "  Test Accuracy: 0.5742753623188406\n",
      "  Validation Precision: 0.46794871794871795\n",
      "  Test Precision: 0.42\n",
      "  Validation Recall: 0.09871534820824882\n",
      "  Test Recall: 0.09251101321585903\n",
      "  Validation F1 Score: 0.16303740926856505\n",
      "  Test F1 Score: 0.15162454873646208\n",
      "  Validation AUC: 0.5145568355297009\n",
      "  Test AUC: 0.5016401219925449\n",
      "\n",
      "\n",
      "Model: Random Forest Regressor\n",
      "  Validation MSE: 0.20800842221925864\n",
      "  Test MSE: 0.2099563784260081\n",
      "  Validation RMSE: 0.4560794034148644\n",
      "  Test RMSE: 0.4582099719844692\n",
      "  Validation MAE: 0.4129232783693187\n",
      "  Test MAE: 0.41651923740510705\n",
      "  Validation R^2: 0.1195623662339268\n",
      "  Test R^2: 0.13284244890656216\n",
      "\n",
      "\n",
      "Model: Decision Tree Regressor\n",
      "  Validation MSE: 0.41018949850471587\n",
      "  Test MSE: 0.4438405797101449\n",
      "  Validation RMSE: 0.6404603801209844\n",
      "  Test RMSE: 0.6662136141735209\n",
      "  Validation MAE: 0.4104986197377502\n",
      "  Test MAE: 0.4438405797101449\n",
      "  Validation R^2: -0.7362098496113068\n",
      "  Test R^2: -0.833141308031176\n",
      "\n",
      "\n",
      "Model: Gradient Boosting Regressor\n",
      "  Validation MSE: 0.21276309080534594\n",
      "  Test MSE: 0.21846966715950683\n",
      "  Validation RMSE: 0.46126249663867747\n",
      "  Test RMSE: 0.4674073888584848\n",
      "  Validation MAE: 0.43286731166880843\n",
      "  Test MAE: 0.43707342332818055\n",
      "  Validation R^2: 0.09943727170836003\n",
      "  Test R^2: 0.09768103744940193\n",
      "\n",
      "\n",
      "Model: AdaBoost Regressor\n",
      "  Validation MSE: 0.22571988768950987\n",
      "  Test MSE: 0.22710194415745916\n",
      "  Validation RMSE: 0.47509987127919734\n",
      "  Test RMSE: 0.4765521421182148\n",
      "  Validation MAE: 0.44937845181942937\n",
      "  Test MAE: 0.4488976844194174\n",
      "  Validation R^2: 0.044595013552792384\n",
      "  Test R^2: 0.0620281830897399\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define models to test with their corresponding problem types\n",
    "models = {\n",
    "    \"Logistic Regression\": (LogisticRegression(max_iter=1000, random_state=42), \"classification\"),  # Increased max_iter\n",
    "    \"Random Forest Classifier\": (RandomForestClassifier(n_estimators=100, random_state=42), \"classification\"),\n",
    "    \"Decision Tree Classifier\": (DecisionTreeClassifier(random_state=42), \"classification\"),\n",
    "    \"Gradient Boosting Classifier\": (GradientBoostingClassifier(random_state=42), \"classification\"),\n",
    "    \"AdaBoost Classifier\": (AdaBoostClassifier(random_state=42), \"classification\"),\n",
    "    \"Support Vector Classifier\": (SVC(), \"classification\"),\n",
    "    \"K-Nearest Neighbors Classifier\": (KNeighborsClassifier(), \"classification\"),\n",
    "    \"Ridge Classifier\": (RidgeClassifier(), \"classification\"),\n",
    "    \"Random Forest Regressor\": (RandomForestRegressor(n_estimators=100, random_state=42), \"regression\"),\n",
    "    \"Decision Tree Regressor\": (DecisionTreeRegressor(random_state=42), \"regression\"),\n",
    "    \"Gradient Boosting Regressor\": (GradientBoostingRegressor(random_state=42), \"regression\"),\n",
    "    \"AdaBoost Regressor\": (AdaBoostRegressor(random_state=42), \"regression\"),\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, (model, problem_type) in models.items():\n",
    "    if problem_type == \"classification\":\n",
    "        model_fitted, val_accuracy, test_accuracy, val_precision, test_precision, val_recall, test_recall, val_f1, test_f1, val_auc, test_auc = model_data(\n",
    "            model, X_train, y_train, X_val, y_val, X_test, y_test, problem_type=problem_type)\n",
    "        \n",
    "        # Store classification results\n",
    "        results[name] = {\n",
    "            \"Validation Accuracy\": val_accuracy,\n",
    "            \"Test Accuracy\": test_accuracy,\n",
    "            \"Validation Precision\": val_precision,\n",
    "            \"Test Precision\": test_precision,\n",
    "            \"Validation Recall\": val_recall,\n",
    "            \"Test Recall\": test_recall,\n",
    "            \"Validation F1 Score\": val_f1,\n",
    "            \"Test F1 Score\": test_f1,\n",
    "            \"Validation AUC\": val_auc,\n",
    "            \"Test AUC\": test_auc\n",
    "        }\n",
    "    elif problem_type == \"regression\":\n",
    "        model_fitted, val_mse, test_mse, val_rmse, test_rmse, val_mae, test_mae, val_r2, test_r2 = model_data(\n",
    "            model, X_train, y_train, X_val, y_val, X_test, y_test, problem_type=problem_type)\n",
    "        \n",
    "        # Store regression results\n",
    "        results[name] = {\n",
    "            \"Validation MSE\": val_mse,\n",
    "            \"Test MSE\": test_mse,\n",
    "            \"Validation RMSE\": val_rmse,\n",
    "            \"Test RMSE\": test_rmse,\n",
    "            \"Validation MAE\": val_mae,\n",
    "            \"Test MAE\": test_mae,\n",
    "            \"Validation R^2\": val_r2,\n",
    "            \"Test R^2\": test_r2\n",
    "        }\n",
    "\n",
    "# Print results\n",
    "for name, result in results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    for metric, value in result.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7203c4-1932-4074-8542-9bee2cd232a1",
   "metadata": {},
   "source": [
    "### Classification Model Performance Comparison After Capping Outliers\n",
    "\n",
    "In this analysis, we compare the performance of different classification models using the `alpha_gt_3` target variable. The dataset was preprocessed to cap outliers using the 1st and 99th quantiles (`quantile_1=0.01`, `quantile_3=0.99`). The models evaluated include Logistic Regression, Random Forest, Decision Tree, Gradient Boosting, AdaBoost, Support Vector Classifier, K-Nearest Neighbors, and Ridge Classifier.\n",
    "\n",
    "#### Observations\n",
    "\n",
    "- **Random Forest Classifier** remains a strong performer across most metrics, especially in terms of F1 Score and AUC, which are critical for imbalanced datasets.\n",
    "- **Gradient Boosting Classifier** continues to show strong overall performance, particularly in Accuracy.\n",
    "- **Support Vector Classifier** shows high Precision but very low Recall, indicating that it is highly conservative in predicting the positive class.\n",
    "- **K-Nearest Neighbors** and **Decision Tree Classifier** offer a balance between Precision and Recall, though they do not excel in any specific area.\n",
    "\n",
    "#### Recommendations\n",
    "\n",
    "- **Model Selection:** Based on these results, **Random Forest Classifier** and **Gradient Boosting Classifier** remain strong candidates for further tuning and potential deployment, as they offer a good balance of accuracy and robustness across different metrics.\n",
    "- **Hyperparameter Tuning:** We should consider hyperparameter tuning for Random Forest and Gradient Boosting to potentially improve their performance further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd95bfd-05d4-46f9-af3d-3f2b74a7f496",
   "metadata": {},
   "source": [
    "### Conclusion: Dataset Selection for Classification Models\n",
    "\n",
    "#### Overview\n",
    "\n",
    "We compared the performance of several classification models using three different approaches to handling outliers in the dataset:\n",
    "1. **With Outliers:** The original dataset without any modifications to outliers.\n",
    "2. **Capping Outliers:** The dataset where outliers were capped at the 1st and 99th percentiles.\n",
    "3. **Dropping Outliers:** The dataset where outliers beyond the 1st and 99th percentiles were removed.\n",
    "\n",
    "#### Key Findings\n",
    "\n",
    "##### 1. **With Outliers**\n",
    "   - Models struggled with outliers, showing lower F1 scores and inconsistent AUC values across both the validation and test sets.\n",
    "   - The presence of extreme values made it difficult for models to generalize, leading to overfitting in some cases.\n",
    "\n",
    "##### 2. **Capping Outliers**\n",
    "   - **Random Forest Classifier** and **Gradient Boosting Classifier** continued to perform well with capped outliers, maintaining high F1 scores and AUC.\n",
    "   - **Support Vector Classifier** achieved high precision but very low recall, indicating that it was conservative in predicting the positive class.\n",
    "   - Capping outliers generally led to more consistent model performance, particularly in metrics like F1 Score and AUC, which are crucial for imbalanced datasets.\n",
    "\n",
    "##### 3. **Dropping Outliers**\n",
    "   - Removing outliers improved the overall model performance, particularly for models like **Random Forest** and **Gradient Boosting**, which showed higher accuracy and AUC compared to the capped and uncapped datasets.\n",
    "   - However, dropping outliers reduced the sample size, which could potentially limit the model's ability to generalize.\n",
    "\n",
    "#### Recommendations\n",
    "\n",
    "- **Use the Dataset with Capped Outliers:** Based on the results, the dataset with capped outliers at the 1st and 99th percentiles offers the best balance between model performance and data integrity. \n",
    "  - **Random Forest Classifier** and **Gradient Boosting Classifier** showed strong, consistent performance across key metrics such as F1 Score and AUC, making this dataset a reliable choice for further model tuning and deployment.\n",
    "  \n",
    "- **Avoid Dropping Outliers:** Although dropping outliers led to slightly better performance in some models, the reduced sample size could pose a risk of overfitting. Therefore, it is recommended to avoid dropping outliers unless the outliers are extremely impactful.\n",
    "\n",
    "- **Further Steps:** \n",
    "  - Consider hyperparameter tuning, particularly for the Random Forest and Gradient Boosting models, to further enhance their performance.\n",
    "  - Explore feature engineering and dimensionality reduction techniques (e.g., PCA) to see if they can improve model accuracy and robustness.\n",
    "\n",
    "#### Final Decision\n",
    "\n",
    "The dataset with capped outliers is going to be used for future model development and deployment, given its balance of performance and sample integrity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad0b0b-bed9-452a-a15a-0bcfbac1bd22",
   "metadata": {},
   "source": [
    "# 3. Hyper Parameter tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f7229-1690-4efb-bae5-8e3159ea2aa0",
   "metadata": {},
   "source": [
    "Let's create a function for hyper parameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ff3916e-1946-4754-b9ad-eca3696e3da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "def tune_and_evaluate(models, param_grids, X_train, y_train, X_val, y_val, X_test, y_test, search_method='grid', n_iter=10):\n",
    "    \"\"\"\n",
    "    Tunes hyperparameters, trains models, and evaluates them.\n",
    "\n",
    "    Parameters:\n",
    "    - models: dict of models with their corresponding problem types ('classification' or 'regression')\n",
    "    - param_grids: dict of hyperparameter grids for each model\n",
    "    - X_train, y_train: Training data\n",
    "    - X_val, y_val: Validation data\n",
    "    - X_test, y_test: Test data\n",
    "    - search_method: 'grid' for GridSearchCV, 'random' for RandomizedSearchCV\n",
    "    - n_iter: Number of iterations for RandomizedSearchCV (ignored if using GridSearchCV)\n",
    "\n",
    "    Returns:\n",
    "    - best_models: dict of best models and their best hyperparameters\n",
    "    - results: dict of evaluation metrics for each model\n",
    "    \"\"\"\n",
    "    \n",
    "    best_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    for name, (model, problem_type) in models.items():\n",
    "        print(f\"Tuning and evaluating model: {name}\")\n",
    "        \n",
    "        if name in param_grids:\n",
    "            if search_method == 'grid':\n",
    "                search = GridSearchCV(estimator=model, param_grid=param_grids[name], \n",
    "                                      cv=5, scoring='f1' if problem_type == 'classification' else 'neg_mean_squared_error')\n",
    "            elif search_method == 'random':\n",
    "                search = RandomizedSearchCV(estimator=model, param_distributions=param_grids[name], \n",
    "                                            n_iter=n_iter, cv=5, random_state=42, scoring='f1' if problem_type == 'classification' else 'neg_mean_squared_error')\n",
    "            else:\n",
    "                raise ValueError(\"search_method must be 'grid' or 'random'\")\n",
    "            \n",
    "            search.fit(X_train, y_train)\n",
    "            best_model = search.best_estimator_\n",
    "            best_params = search.best_params_\n",
    "        else:\n",
    "            best_model = model\n",
    "            best_params = {}\n",
    "\n",
    "        # Store the best model and its parameters\n",
    "        best_models[name] = (best_model, best_params)\n",
    "\n",
    "        # Evaluate the model\n",
    "        if problem_type == \"classification\":\n",
    "            model_fitted, val_accuracy, test_accuracy, val_precision, test_precision, val_recall, test_recall, val_f1, test_f1, val_auc, test_auc = model_data(\n",
    "                best_model, X_train, y_train, X_val, y_val, X_test, y_test, problem_type=problem_type)\n",
    "            \n",
    "            # Store classification results\n",
    "            results[name] = {\n",
    "                \"Validation Accuracy\": val_accuracy,\n",
    "                \"Test Accuracy\": test_accuracy,\n",
    "                \"Validation Precision\": val_precision,\n",
    "                \"Test Precision\": test_precision,\n",
    "                \"Validation Recall\": val_recall,\n",
    "                \"Test Recall\": test_recall,\n",
    "                \"Validation F1 Score\": val_f1,\n",
    "                \"Test F1 Score\": test_f1,\n",
    "                \"Validation AUC\": val_auc,\n",
    "                \"Test AUC\": test_auc\n",
    "            }\n",
    "        \n",
    "        elif problem_type == \"regression\":\n",
    "            model_fitted, val_mse, test_mse, val_rmse, test_rmse, val_mae, test_mae, val_r2, test_r2 = model_data(\n",
    "                best_model, X_train, y_train, X_val, y_val, X_test, y_test, problem_type=problem_type)\n",
    "            \n",
    "            # Store regression results\n",
    "            results[name] = {\n",
    "                \"Validation MSE\": val_mse,\n",
    "                \"Test MSE\": test_mse,\n",
    "                \"Validation RMSE\": val_rmse,\n",
    "                \"Test RMSE\": test_rmse,\n",
    "                \"Validation MAE\": val_mae,\n",
    "                \"Test MAE\": test_mae,\n",
    "                \"Validation R^2\": val_r2,\n",
    "                \"Test R^2\": test_r2\n",
    "            }\n",
    "\n",
    "        print(f\"Best Parameters for {name}: {best_params}\\n\")\n",
    "\n",
    "    return best_models, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd0f883-f82d-48b3-ac93-59e63646900d",
   "metadata": {},
   "source": [
    "Let's execute the hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0d681-57d4-4cba-a198-54095b18b1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning and evaluating model: Random Forest Classifier\n",
      "Best Parameters for Random Forest Classifier: {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\n",
      "Tuning and evaluating model: Gradient Boosting Classifier\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Random Forest Classifier\": (RandomForestClassifier(random_state=42), \"classification\"),\n",
    "    \"Gradient Boosting Classifier\": (GradientBoostingClassifier(random_state=42), \"classification\")\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"Random Forest Classifier\": {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    \"Gradient Boosting Classifier\": {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Call the function with 'grid' search\n",
    "#best_models_grid, results_grid = tune_and_evaluate(models, param_grids, X_train, y_train, X_val, y_val, X_test, y_test, search_method='grid')\n",
    "\n",
    "# Call the function with 'random' search\n",
    "best_models_random, results_random = tune_and_evaluate(models, param_grids, X_train, y_train, X_val, y_val, X_test, y_test, search_method='random', n_iter=20)\n",
    "\n",
    "# Print results for grid search\n",
    "for name, result in best_models_grid.items():\n",
    "    print(f\"Best Model (Grid Search): {name}\")\n",
    "    print(f\"  Best Parameters: {result[1]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "for name, result in results_grid.items():\n",
    "    print(f\"Model (Grid Search): {name}\")\n",
    "    for metric, value in result.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print results for random search\n",
    "for name, result in best_models_random.items():\n",
    "    print(f\"Best Model (Random Search): {name}\")\n",
    "    print(f\"  Best Parameters: {result[1]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "for name, result in results_random.items():\n",
    "    print(f\"Model (Random Search): {name}\")\n",
    "    for metric, value in result.items():\n",
    "        print(f\"  {metric}: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eee45e3-f376-4c0a-aab2-4eeb920ca308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
